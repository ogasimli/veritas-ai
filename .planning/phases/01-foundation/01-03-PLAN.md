---
phase: 01-foundation
plan: 03
type: execute
---

<objective>
Create database models and GCS storage service.

Purpose: Define the data layer for jobs, documents, and findings that agents will populate.
Output: SQLAlchemy models, Alembic migrations, and GCS upload/download service.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
@backend/app/models/base.py
@backend/app/db.py

**Tech stack (from PROJECT.md):**
- Database: PostgreSQL with SQLAlchemy async
- Storage: Google Cloud Storage for document files
- Models needed: Job (processing session), Document (uploaded file), Finding (detected issue)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SQLAlchemy models</name>
  <files>backend/app/models/job.py, backend/app/models/document.py, backend/app/models/finding.py, backend/app/models/__init__.py</files>
  <action>
Create domain models:

1. backend/app/models/job.py - Job model:
   - id: UUID primary key (use uuid.uuid4 default)
   - status: String (pending, processing, completed, failed)
   - created_at: DateTime (server_default=func.now())
   - updated_at: DateTime (onupdate=func.now())
   - error_message: String nullable (for failed jobs)
   - Relationship to documents and findings

2. backend/app/models/document.py - Document model:
   - id: UUID primary key
   - job_id: UUID foreign key to Job
   - filename: String (original filename)
   - gcs_path: String (path in GCS bucket)
   - content_type: String
   - extracted_text: Text nullable (populated after extraction)
   - created_at: DateTime

3. backend/app/models/finding.py - Finding model:
   - id: UUID primary key
   - job_id: UUID foreign key to Job
   - category: String (numeric, logic, disclosure, external)
   - severity: String (high, medium, low)
   - description: Text
   - source_refs: JSON (list of source references)
   - reasoning: Text nullable (for non-numeric findings)
   - agent_id: String (which agent produced this)
   - created_at: DateTime

4. Update backend/app/models/__init__.py to export all models

Use SQLAlchemy 2.0 style: Mapped[], mapped_column(), relationship(). Use UUID type from sqlalchemy.dialects.postgresql.
  </action>
  <verify>cd backend && python -c "from app.models import Job, Document, Finding; print('Models loaded:', Job.__tablename__, Document.__tablename__, Finding.__tablename__)"</verify>
  <done>All three models importable, relationships defined, proper types used</done>
</task>

<task type="auto">
  <name>Task 2: Set up GCS storage service</name>
  <files>backend/app/services/__init__.py, backend/app/services/storage.py</files>
  <action>
Create GCS service for document storage:

1. backend/app/services/__init__.py: empty

2. backend/app/services/storage.py:
   - StorageService class
   - __init__(self, bucket_name: str) - creates GCS client and gets bucket
   - async upload_file(self, file_content: bytes, destination_path: str, content_type: str) -> str
     - Uploads to GCS, returns full gs:// path
   - async download_file(self, gcs_path: str) -> bytes
     - Downloads file content from GCS
   - async delete_file(self, gcs_path: str) -> None
     - Deletes file from GCS
   - get_storage_service() dependency function using settings.GCS_BUCKET

Use google-cloud-storage library. Handle case where GCS_BUCKET is empty (dev mode) - in that case, store files locally in backend/uploads/ directory instead. This allows local development without GCS credentials.

Import: from google.cloud import storage
  </action>
  <verify>cd backend && python -c "from app.services.storage import StorageService; print('Storage service loaded')"</verify>
  <done>StorageService class created with upload/download/delete methods, fallback to local storage for dev</done>
</task>

<task type="auto">
  <name>Task 3: Set up Alembic migrations</name>
  <files>backend/alembic.ini, backend/alembic/env.py, backend/alembic/versions/.gitkeep</files>
  <action>
Initialize Alembic for database migrations:

1. cd backend && alembic init alembic

2. Update backend/alembic.ini:
   - Set sqlalchemy.url to empty (will be set from env)

3. Update backend/alembic/env.py:
   - Import settings from app.config
   - Import Base from app.models.base
   - Import all models (Job, Document, Finding) to register them
   - Set target_metadata = Base.metadata
   - Configure sqlalchemy.url from settings.DATABASE_URL
   - Use async engine for run_migrations_online()

4. Create initial migration:
   cd backend && alembic revision --autogenerate -m "initial_schema"

Note: Alembic with async requires some setup. Use run_sync() pattern in env.py for the migration context. See SQLAlchemy docs for async Alembic pattern.
  </action>
  <verify>cd backend && alembic check 2>&1 | head -5 || echo "Alembic configured (may need DB to fully verify)"</verify>
  <done>Alembic initialized, env.py configured for async, initial migration created</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All models import without errors
- [ ] Models have proper relationships (Job -> Documents, Job -> Findings)
- [ ] StorageService handles both GCS and local fallback
- [ ] Alembic configured with async support
- [ ] Initial migration file exists in alembic/versions/
</verification>

<success_criteria>
- All tasks completed
- Job, Document, Finding models defined with proper relationships
- GCS service ready (with local fallback for dev)
- Alembic migrations initialized
- Phase 1: Foundation complete
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`

**Phase 1 complete** - Update ROADMAP.md progress table and STATE.md
</output>
