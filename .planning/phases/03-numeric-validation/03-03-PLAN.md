---
phase: 03-numeric-validation
plan: 03-03
type: execute
domain: agents
---

<objective>
Create the ReviewerAgent that filters passing checks, re-verifies failures using code execution, generates summaries, assigns severity, deduplicates findings, and integrates the pipeline with the document processing service.

Purpose: Complete the numeric validation pipeline with quality-assured findings output.
Output: Working end-to-end pipeline that processes documents and produces validated findings.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-numeric-validation/DISCOVERY.md
@.planning/phases/03-numeric-validation/03-RESEARCH.md
@.planning/phases/03-numeric-validation/03-01-PLAN.md
@.planning/phases/03-numeric-validation/03-02-PLAN.md
@backend/app/models/finding.py
@backend/app/models/job.py
@backend/agents/numeric_validation/sub_agents/fan_out_verifier/schema.py

**Tech stack available:**
- SequentialAgent for pipeline orchestration
- BuiltInCodeExecutor for re-verification
- Verifier checks stored in session.state['checks:*'] keys
- Finding model with category, severity, description, source_refs, reasoning, agent_id

**Pattern from DISCOVERY.md:**
- Reviewer reads all checks:* keys from session state
- Uses code_executor to re-verify failures
- Generates summary from check details (description, expected_value, actual_value, source_refs)
- Assigns severity: high (>5%), medium (1-5%), low (<1%)

**Constraining decisions:**
- BuiltInCodeExecutor can ONLY be used by itself in an agent
- Reviewer must deduplicate similar findings
- Output findings ready for database storage
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Finding schema for ReviewerAgent output</name>
  <files>backend/agents/numeric_validation/sub_agents/reviewer/schema.py</files>
  <action>
1. Create directory: backend/agents/numeric_validation/sub_agents/reviewer/
2. Create schema.py with Finding and ReviewerAgentOutput:
   ```python
   from typing import List
   from pydantic import BaseModel

   class Finding(BaseModel):
       fsli_name: str
       summary: str                 # Short human-readable summary
       severity: str                # "high" | "medium" | "low"
       expected_value: float
       actual_value: float
       discrepancy: float           # Absolute difference
       source_refs: List[str]       # ["Table 4, Row 12", "Note 5"]

   class ReviewerAgentOutput(BaseModel):
       findings: List[Finding]
   ```

Avoid: Don't include code_executed in Finding - that's internal to verification.
  </action>
  <verify>python -c "from agents.numeric_validation.sub_agents.reviewer.schema import Finding, ReviewerAgentOutput; print('schema ok')"</verify>
  <done>Finding schema defined with summary and severity fields</done>
</task>

<task type="auto">
  <name>Task 2: Create Reviewer prompt</name>
  <files>backend/agents/numeric_validation/sub_agents/reviewer/prompt.py</files>
  <action>
1. Create prompt.py with INSTRUCTION:
   ```python
   INSTRUCTION = """You are a financial findings reviewer. Your job is to review verification results and produce final findings.

   ## Input

   You receive verification checks from session state (keys starting with "checks:").
   Each check has: fsli_name, check_type, description, expected_value, actual_value, result, source_refs, code_executed.

   ## Your Tasks

   1. **Filter**: Only process checks with result="fail". Ignore passing checks.

   2. **Re-verify**: For each failure, use Python code execution to confirm the discrepancy is real.
      - Re-calculate: expected_value vs actual_value
      - If re-verification shows the check should pass, skip it
      - If confirmed failure, proceed to next step

   3. **Generate Summary**: Create a concise human-readable summary for each confirmed failure.
      - Include the FSLI name
      - Describe what was expected vs what was found
      - Reference the source location
      - Example: "Revenue components (Product: $1M + Service: $500K) do not sum to reported Total Revenue ($1.6M). Discrepancy: $100K in Table 4."

   4. **Assign Severity**: Based on discrepancy percentage:
      - "high": Discrepancy > 5% of expected value
      - "medium": Discrepancy between 1% and 5%
      - "low": Discrepancy < 1%

   5. **Deduplicate**: If multiple checks report the same underlying issue, keep only one finding.
      - Same FSLI + same source_refs = likely duplicate
      - Choose the finding with the clearest summary

   6. **Output**: Return structured Finding objects with all required fields.

   ## Code Execution

   Use Python for ALL re-verification calculations. Be precise with floating-point comparisons.

   Example re-verification:
   ```python
   expected = 1500000
   actual = 1600000
   discrepancy = abs(expected - actual)
   percentage = (discrepancy / expected) * 100
   print(f"Discrepancy: {discrepancy}, Percentage: {percentage:.2f}%")
   ```
   """
   ```

Avoid: Don't include passing checks in output.
  </action>
  <verify>python -c "from agents.numeric_validation.sub_agents.reviewer.prompt import INSTRUCTION; print(len(INSTRUCTION) > 100)"</verify>
  <done>Reviewer prompt created with re-verification and severity assignment logic</done>
</task>

<task type="auto">
  <name>Task 3: Create ReviewerAgent</name>
  <files>backend/agents/numeric_validation/sub_agents/reviewer/agent.py, backend/agents/numeric_validation/sub_agents/reviewer/__init__.py</files>
  <action>
1. Create agent.py:
   ```python
   """ReviewerAgent - filters, re-verifies, and outputs final findings."""
   from google.adk.agents import LlmAgent
   from google.adk.code_executors import BuiltInCodeExecutor

   from . import prompt
   from .schema import ReviewerAgentOutput

   reviewer_agent = LlmAgent(
       name="ReviewerAgent",
       model="gemini-3-pro-preview",
       instruction=prompt.INSTRUCTION,
       output_key="reviewer_output",
       output_schema=ReviewerAgentOutput,
       code_executor=BuiltInCodeExecutor(),  # For re-verification
   )
   ```

2. Create __init__.py:
   ```python
   """Reviewer sub-agent exports."""
   from .agent import reviewer_agent
   from .schema import Finding, ReviewerAgentOutput
   ```

Avoid: Don't add other tools - code_executor must be exclusive.
  </action>
  <verify>python -c "from agents.numeric_validation.sub_agents.reviewer import reviewer_agent; print(reviewer_agent.name)"</verify>
  <done>ReviewerAgent created with code_executor for re-verification</done>
</task>

<task type="auto">
  <name>Task 4: Complete root agent pipeline</name>
  <files>backend/agents/numeric_validation/agent.py, backend/agents/numeric_validation/sub_agents/__init__.py</files>
  <action>
1. Update sub_agents/__init__.py:
   ```python
   """Sub-agents for numeric validation."""
   from .extractor import extractor_agent
   from .fan_out_verifier import fan_out_verifier_agent
   from .reviewer import reviewer_agent
   ```

2. Update agent.py:
   ```python
   """Root agent definition."""
   from google.adk.agents import SequentialAgent
   from .sub_agents import extractor_agent, fan_out_verifier_agent, reviewer_agent

   root_agent = SequentialAgent(
       name='numeric_validation',
       description='Pipeline for financial statement numeric validation',
       sub_agents=[
           extractor_agent,        # Extract FSLI names
           fan_out_verifier_agent, # Parallel verification per FSLI
           reviewer_agent,         # Filter, re-verify, output findings
       ],
   )
   ```
  </action>
  <verify>python -c "from agents.numeric_validation.agent import root_agent; print(len(root_agent.sub_agents))" shows 3</verify>
  <done>Complete pipeline: Extractor → FanOutVerifier → Reviewer</done>
</task>

<task type="auto">
  <name>Task 5: Integrate pipeline into document processing service</name>
  <files>backend/app/services/processor.py, backend/app/api/routes/documents.py</files>
  <action>
1. Create backend/app/services/processor.py:
   ```python
   """Document processing service with agent pipeline integration."""
   from uuid import UUID
   from sqlalchemy.ext.asyncio import AsyncSession
   from google.adk.runners import InMemoryRunner
   from google.genai.types import Part, UserContent

   from agents.numeric_validation.agent import root_agent
   from app.models.finding import Finding as FindingModel
   from app.models.job import Job


   class DocumentProcessor:
       """Processes documents through the numeric validation pipeline."""

       def __init__(self, db: AsyncSession):
           self.db = db

       async def process_document(self, job_id: UUID, extracted_text: str) -> None:
           """Run numeric validation pipeline and save findings."""
           # 1. Update job status
           job = await self.db.get(Job, job_id)
           if not job:
               raise ValueError(f"Job {job_id} not found")

           job.status = "processing"
           await self.db.commit()

           try:
               # 2. Run pipeline
               runner = InMemoryRunner(agent=root_agent, app_name="veritas-ai")
               session = await runner.session_service.create_session(
                   app_name=runner.app_name,
                   user_id=str(job_id)
               )

               content = UserContent(parts=[Part(text=extracted_text)])

               # Collect final response
               final_state = {}
               async for event in runner.run_async(
                   user_id=session.user_id,
                   session_id=session.id,
                   new_message=content,
               ):
                   # Capture session state updates
                   if hasattr(event, 'session') and event.session:
                       final_state = event.session.state

               # 3. Extract findings from reviewer output
               reviewer_output = final_state.get("reviewer_output", {})
               findings = reviewer_output.get("findings", [])

               # 4. Save findings to database
               for finding_data in findings:
                   finding = FindingModel(
                       job_id=job_id,
                       category="numeric",
                       severity=finding_data.get("severity", "medium"),
                       description=finding_data.get("summary", ""),
                       source_refs=finding_data.get("source_refs", []),
                       reasoning=f"Expected: {finding_data.get('expected_value')}, "
                                f"Actual: {finding_data.get('actual_value')}, "
                                f"Discrepancy: {finding_data.get('discrepancy')}",
                       agent_id="numeric_validation",
                   )
                   self.db.add(finding)

               # 5. Update job status
               job.status = "completed"
               await self.db.commit()

           except Exception as e:
               job.status = "failed"
               job.error_message = str(e)
               await self.db.commit()
               raise
   ```

2. Update backend/app/api/routes/documents.py to use DocumentProcessor after extraction (if not already integrated - check existing code first).

3. Export DocumentProcessor from services/__init__.py if exists.

Avoid: Don't add WebSocket updates yet (Phase 7). Don't overwrite existing integration logic - extend it.
  </action>
  <verify>cd backend && python -c "from app.services.processor import DocumentProcessor; print('processor ok')"</verify>
  <done>Document upload triggers extraction → validation pipeline → findings saved to DB</done>
</task>

<task type="auto">
  <name>Task 6: Add unit tests for ReviewerAgent</name>
  <files>backend/agents/numeric_validation/tests/test_reviewer.py</files>
  <action>
1. Create test_reviewer.py:
   ```python
   """Tests for ReviewerAgent."""
   import pytest
   import dotenv

   from agents.numeric_validation.sub_agents.reviewer import (
       reviewer_agent,
       Finding,
       ReviewerAgentOutput,
   )

   pytest_plugins = ("pytest_asyncio",)

   @pytest.fixture(scope="session", autouse=True)
   def load_env():
       dotenv.load_dotenv()

   def test_reviewer_agent_structure():
       """Verify ReviewerAgent configuration."""
       assert reviewer_agent.name == "ReviewerAgent"
       assert reviewer_agent.model == "gemini-3-pro-preview"
       assert reviewer_agent.code_executor is not None
       assert reviewer_agent.output_key == "reviewer_output"

   def test_finding_schema():
       """Verify Finding schema."""
       finding = Finding(
           fsli_name="Revenue",
           summary="Revenue components do not sum to total. Discrepancy: $100K.",
           severity="high",
           expected_value=1500000.0,
           actual_value=1600000.0,
           discrepancy=100000.0,
           source_refs=["Table 4, Row 12"],
       )
       assert finding.severity == "high"
       assert finding.discrepancy == 100000.0

   def test_reviewer_agent_output_schema():
       """Verify ReviewerAgentOutput schema."""
       output = ReviewerAgentOutput(findings=[])
       assert output.findings == []

   def test_severity_thresholds():
       """Document severity threshold logic."""
       # These thresholds should match the prompt
       # high: > 5%, medium: 1-5%, low: < 1%
       expected = 1000000
       high_discrepancy = 60000    # 6%
       medium_discrepancy = 30000  # 3%
       low_discrepancy = 5000      # 0.5%

       assert (high_discrepancy / expected) * 100 > 5
       assert 1 < (medium_discrepancy / expected) * 100 < 5
       assert (low_discrepancy / expected) * 100 < 1
   ```
  </action>
  <verify>cd backend && export PYTHONPATH=$(pwd) && ./.venv/bin/pytest agents/numeric_validation/tests/test_reviewer.py -v</verify>
  <done>Unit tests pass for ReviewerAgent structure</done>
</task>

<task type="auto">
  <name>Task 7: Run full pipeline test</name>
  <files>backend/agents/numeric_validation/tests/test_pipeline.py</files>
  <action>
1. Create test_pipeline.py:
   ```python
   """Integration tests for the full numeric validation pipeline."""
   import pytest
   import dotenv
   from google.adk.runners import InMemoryRunner
   from google.genai.types import Part, UserContent

   from agents.numeric_validation.agent import root_agent

   pytest_plugins = ("pytest_asyncio",)

   @pytest.fixture(scope="session", autouse=True)
   def load_env():
       dotenv.load_dotenv()

   def test_root_agent_structure():
       """Verify complete pipeline structure."""
       assert root_agent.name == "numeric_validation"
       assert len(root_agent.sub_agents) == 3

       agent_names = [a.name for a in root_agent.sub_agents]
       assert "ExtractorAgent" in agent_names
       assert "FanOutVerifierAgent" in agent_names
       assert "ReviewerAgent" in agent_names

   def test_pipeline_can_initialize():
       """Verify pipeline can be loaded into a runner."""
       runner = InMemoryRunner(agent=root_agent, app_name="test")
       assert runner.agent.name == "numeric_validation"
   ```

Avoid: Don't run actual LLM calls in CI tests - mock or skip in CI.
  </action>
  <verify>cd backend && export PYTHONPATH=$(pwd) && ./.venv/bin/pytest agents/numeric_validation/tests/test_pipeline.py -v</verify>
  <done>Full pipeline structure tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] backend/agents/numeric_validation/sub_agents/reviewer/ exists with all files
- [ ] ReviewerAgent uses code_executor for re-verification
- [ ] Finding schema has summary and severity fields
- [ ] Root agent has 3 sub-agents: Extractor → FanOutVerifier → Reviewer
- [ ] backend/app/services/processor.py integrates with root_agent
- [ ] Pipeline outputs findings ready for database storage
- [ ] **Unit Tests**: `export PYTHONPATH=$(pwd) && ./.venv/bin/pytest agents/numeric_validation/tests` passes
- [ ] **ADK Run**: `export PYTHONPATH=$(pwd) && ./.venv/bin/adk run agents/numeric_validation` works
</verification>

<success_criteria>
- All tasks completed
- End-to-end flow: upload → extract → extractor → verifiers → reviewer → findings in DB
- ReviewerAgent filters passes, re-verifies failures, generates summaries
- Severity assigned based on discrepancy percentage
- Findings deduplicated before output
- Job status updated throughout processing
- Phase 3 complete: Numeric Validation pipeline operational
</success_criteria>

<output>
After completion, create `.planning/phases/03-numeric-validation/03-03-SUMMARY.md`:

# Phase 03 Plan 03: ReviewerAgent + Pipeline Integration Summary

**[Substantive one-liner about what was built]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]
- [Key outcome 3]

## Files Created/Modified

- `backend/agents/numeric_validation/sub_agents/reviewer/agent.py` - ReviewerAgent with code_executor
- `backend/agents/numeric_validation/sub_agents/reviewer/prompt.py` - Review and re-verification prompt
- `backend/agents/numeric_validation/sub_agents/reviewer/schema.py` - Finding schema with summary/severity
- `backend/agents/numeric_validation/agent.py` - Complete 3-agent pipeline
- `backend/app/services/processor.py` - DB integration service

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Pipeline Architecture

```
Document Text
     ↓
[ExtractorAgent] → fsli_names: ["Revenue", "Cost of Sales", ...]
     ↓
[FanOutVerifierAgent] → Parallel VerifierAgents (one per FSLI)
     ↓                   → checks:Revenue, checks:Cost_of_Sales, ...
[ReviewerAgent] → Filter passes, re-verify fails, assign severity
     ↓
findings: [{summary, severity, source_refs}, ...]
     ↓
Database (Finding model)
```

## Next Step

Phase 3 complete. Ready for Phase 4: Logic Consistency Agent.
</output>
